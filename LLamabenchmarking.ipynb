{
 "cells": [
  {
   "cell_type": "code",
   "id": "4547fc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:08:42.613392Z",
     "start_time": "2025-10-08T20:08:41.569030Z"
    }
   },
   "source": [
    "import ollama\n",
    "import os\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:08:43.272328Z",
     "start_time": "2025-10-08T20:08:42.613392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# warmup\n",
    "# context 4K\n",
    "# Internet Access NO\n",
    "# System : Nvidia gpu\n",
    "output = ollama.chat(\n",
    "    model=\"llama3\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"This is a test message, return a simple 1 line answer if you understand this message\"}\n",
    "    ]\n",
    ")\n",
    "outputs = []\n",
    "output"
   ],
   "id": "c3521205cdc0b8fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3', created_at='2025-10-08T20:08:43.2633508Z', done=True, done_reason='stop', total_duration=636285900, load_duration=97811800, prompt_eval_count=28, prompt_eval_duration=47622100, eval_count=7, eval_duration=490346500, message=Message(role='assistant', content='\"I understand the test message!\"', thinking=None, images=None, tool_name=None, tool_calls=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:08:43.303968Z",
     "start_time": "2025-10-08T20:08:43.275377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO - Input Variations\n",
    "# Different batch size inputs (1, 16, 128)\n",
    "# Different Input tokens length (<20, <100)\n",
    "# Different Complexity inputs (easy, hard)\n",
    "# Different Output length (short, long)\n",
    "# Different Systems (Mac, Windows)"
   ],
   "id": "cca765027d2ee185",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1ca2a3cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:08:44.528845Z",
     "start_time": "2025-10-08T20:08:43.310893Z"
    }
   },
   "source": [
    "def bench(prompts,prefix = \"\", stream = False):\n",
    "    result = {}\n",
    "    output = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prefix},\n",
    "        ] + [{\"role\": \"user\", \"content\": prompt+\"\\n\"} for prompt in prompts],\n",
    "        stream = stream\n",
    "    )\n",
    "\n",
    "    result[\"prompts\"] = len(prompts)\n",
    "    result[\"input_tokens\"] = output.prompt_eval_count\n",
    "    result[\"output_tokens\"] = output.eval_count\n",
    "    result[\"total_time\"] = output.total_duration/1e+6\n",
    "    result[\"load_time\"] = output.load_duration/1e+6\n",
    "    result[\"prompt_eval_time\"] = output.prompt_eval_duration/1e+6\n",
    "    result[\"output_eval_time\"] = output.eval_duration/1e+6\n",
    "\n",
    "    return result, output.message.content\n",
    "\n",
    "bench([\"Do you know whos dashing, smart and disappears in a flash\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'prompts': 1,\n",
       "  'input_tokens': 25,\n",
       "  'output_tokens': 45,\n",
       "  'total_time': 1205.0924,\n",
       "  'load_time': 106.7844,\n",
       "  'prompt_eval_time': 12.1352,\n",
       "  'output_eval_time': 1083.8178},\n",
       " 'I think I do! Is the answer \"Zorro\"? The legendary masked hero created by Johnston McCulley, known for his sword-fighting skills, cleverness, and ability to vanish quickly (in a flash!)')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-08T20:08:44.542243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = os.listdir(\"data/prompts/\")\n",
    "batch_sizes = [1,8,32,128]\n",
    "device = \"RTX3060\"\n",
    "for filename in files:\n",
    "    _, questions, prompt_complexity, prompt_length = filename.split(\".\")[0].split(\"_\")\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        with open(f\"./data/prompts/{filename}\", \"r\") as file:\n",
    "            prompts = file.read().split(\"\\\\n\")[:batch_size]\n",
    "\n",
    "            for i in range(len(prompts)):\n",
    "                prompts[i] = f\"Question {i+1} : {prompts[i].strip()}\"\n",
    "\n",
    "        for output_length in [\"short\", \"long\"]:\n",
    "            if output_length == \"short\":\n",
    "                benchmark, answer = bench(prompts, prefix = f\"give short answers for each of the following {len(prompts)} questions\")\n",
    "            else:\n",
    "                benchmark, answer = bench(prompts, prefix = f\"give long answers for each of the following {len(prompts)} questions\")\n",
    "\n",
    "            with open(f\"./data/answers/answer_{batch_size}_{prompt_complexity}_{prompt_length}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(answer)\n",
    "\n",
    "            benchmark[\"prompt_complexity\"] = prompt_complexity\n",
    "            benchmark[\"prompt_length\"] = prompt_length\n",
    "            benchmark[\"output_length\"] = output_length\n",
    "            benchmark[\"device\"] = device\n",
    "\n",
    "            outputs.append(benchmark)\n",
    "            outputs_df = pd.DataFrame(outputs)\n",
    "            outputs_df.to_csv(f\"./data/{device}_outputs.csv\", index = False)"
   ],
   "id": "7a135d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:08:37.399002800Z",
     "start_time": "2025-10-08T20:05:09.707982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs_df = pd.DataFrame(outputs)\n",
    "outputs_df.to_csv(f\"./data/{device}outputs.csv\", index = False)"
   ],
   "id": "737ad44b4b597d4",
   "outputs": [],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
