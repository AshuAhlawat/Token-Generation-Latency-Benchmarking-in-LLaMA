{
 "cells": [
  {
   "cell_type": "code",
   "id": "4547fc90",
   "metadata": {},
   "source": [
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "device = \"RTX4060\"\n",
    "batch_sizes = [1,8,32,128]\n",
    "outputs = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ca2a3cf",
   "metadata": {},
   "source": [
    "# warmup\n",
    "# context 32K\n",
    "# Internet Access NO\n",
    "\n",
    "def bench(prompts,prefix = \"\", stream = False):\n",
    "    result = {}\n",
    "    output = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prefix},\n",
    "        ] + [{\"role\": \"user\", \"content\": prompt+\"\\n\"} for prompt in prompts],\n",
    "        stream = stream\n",
    "    )\n",
    "\n",
    "    result[\"prompts\"] = len(prompts)\n",
    "    result[\"input_tokens\"] = output.prompt_eval_count\n",
    "    result[\"output_tokens\"] = output.eval_count\n",
    "    result[\"total_time\"] = output.total_duration/1e+6\n",
    "    result[\"load_time\"] = output.load_duration/1e+6\n",
    "    result[\"prompt_eval_time\"] = output.prompt_eval_duration/1e+6\n",
    "    result[\"output_eval_time\"] = output.eval_duration/1e+6\n",
    "\n",
    "    return result, output.message.content\n",
    "\n",
    "bench([\"Do you know whos dashing, smart and disappears in a flash\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files = os.listdir(\"data/prompts/\")\n",
    "\n",
    "# iterating over different files full of variations of questions\n",
    "for filename in files:\n",
    "    # getting some data about the question out of the name of the file\n",
    "    _, questions, prompt_complexity, prompt_length = filename.split(\".\")[0].split(\"_\")\n",
    "\n",
    "    # iterating over different batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        with open(f\"./data/prompts/{filename}\", \"r\") as file:\n",
    "            all_prompts = file.read().split(\"\\\\n\")\n",
    "\n",
    "            for i in range(len(all_prompts)):\n",
    "                all_prompts[i] = f\"Question {i+1} : {all_prompts[i].strip()}\"\n",
    "        # iterating over the prompts in the files in batches of batch_size\n",
    "        for i in range(0, len(all_prompts)-1, batch_size):\n",
    "            prompts = all_prompts[i: i+batch_size]\n",
    "            # iterating over different prefix expecting the output length to be different\n",
    "            for output_length in [\"short\", \"long\"]:\n",
    "                if output_length == \"short\":\n",
    "                    benchmark, answer = bench(prompts, prefix = f\"give very short answers to the questions, if answers get longer than 50 words stop\")\n",
    "                else:\n",
    "                    benchmark, answer = bench(prompts, prefix = f\"give long answers to the questions, if answers get longer than 500 words stop\")\n",
    "                # saving\n",
    "                with open(f\"./data/answers/answer_{batch_size}_{prompt_complexity}_{prompt_length}.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(answer)\n",
    "\n",
    "                benchmark[\"prompt_complexity\"] = prompt_complexity\n",
    "                benchmark[\"prompt_length\"] = prompt_length\n",
    "                benchmark[\"output_length\"] = output_length\n",
    "                benchmark[\"device\"] = device\n",
    "\n",
    "                outputs.append(benchmark)\n",
    "                outputs_df = pd.DataFrame(outputs)\n",
    "                outputs_df.to_csv(f\"./data/{device}_outputs.csv\", index = False)"
   ],
   "id": "7a135d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outputs_df = pd.read_csv(f\"./data/{device}_outputs.csv\")\n",
    "outputs_df"
   ],
   "id": "8cff8bcb97ab3937",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
