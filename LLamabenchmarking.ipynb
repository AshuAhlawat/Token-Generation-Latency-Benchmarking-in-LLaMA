{
 "cells": [
  {
   "cell_type": "code",
   "id": "4547fc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:39:51.248287Z",
     "start_time": "2025-10-06T19:39:50.298750Z"
    }
   },
   "source": "import ollama",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:39:52.270254Z",
     "start_time": "2025-10-06T19:39:51.248287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# warmup\n",
    "# context 4K\n",
    "# Internet Access NO\n",
    "# System : Nvidia gpu\n",
    "output = ollama.chat(\n",
    "    model=\"llama3\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"This is a test message, return a simple 1 line answer if you understand this message\"}\n",
    "    ]\n",
    ")\n",
    "output"
   ],
   "id": "c3521205cdc0b8fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3', created_at='2025-10-06T19:39:52.2570456Z', done=True, done_reason='stop', total_duration=991024400, load_duration=114897700, prompt_eval_count=28, prompt_eval_duration=589458300, eval_count=4, eval_duration=286163200, message=Message(role='assistant', content='Got it!', thinking=None, images=None, tool_name=None, tool_calls=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:39:52.285916Z",
     "start_time": "2025-10-06T19:39:52.270254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO - Input Variations\n",
    "# Different batch size inputs (1, 16, 128)\n",
    "# Different Input tokens length (<20, <100)\n",
    "# Different Complexity inputs (easy, hard)\n",
    "# Different Output length (short, long)\n",
    "# Different Systems (Mac, Windows)"
   ],
   "id": "cca765027d2ee185",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:39:52.349048Z",
     "start_time": "2025-10-06T19:39:52.333170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "result_headers = {\"batch size\", \"complexity\", \"device\", \"chunk size\",\"per token latency\", \"total latency\", \"throughput\"}\n",
    "tokens = {\"token\",\"time\"}"
   ],
   "id": "575f460056b3c8f0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "1ca2a3cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:39:52.380655Z",
     "start_time": "2025-10-06T19:39:52.364681Z"
    }
   },
   "source": [
    "def bench(prompts,prefix = \"\", stream = False):\n",
    "    result = {}\n",
    "    output = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prefix},\n",
    "        ] + [{\"role\": \"user\", \"content\": prompt+\"\\n\"} for prompt in prompts],\n",
    "        stream = stream\n",
    "    )\n",
    "\n",
    "    result[\"prompts\"] = len(prompts)\n",
    "    result[\"input_tokens\"] = output.prompt_eval_count\n",
    "    result[\"output_tokens\"] = output.eval_count\n",
    "    result[\"total_time\"] = output.total_duration/1e+6\n",
    "    result[\"load_time\"] = output.load_duration/1e+6\n",
    "    result[\"prompt_eval_time\"] = output.prompt_eval_duration/1e+6\n",
    "    result[\"output_eval_time\"] = output.eval_duration/1e+6\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:39:52.891862Z",
     "start_time": "2025-10-06T19:39:52.386753Z"
    }
   },
   "cell_type": "code",
   "source": "bench([\"Do you know whos dashing, smart and disappears in a flash\"])",
   "id": "7a135d55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompts': 1,\n",
       " 'input_tokens': 25,\n",
       " 'output_tokens': 16,\n",
       " 'total_time': 487.3589,\n",
       " 'load_time': 123.0068,\n",
       " 'prompt_eval_time': 22.9218,\n",
       " 'output_eval_time': 337.9171}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
