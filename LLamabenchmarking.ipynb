{
 "cells": [
  {
   "cell_type": "code",
   "id": "4547fc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T04:15:33.082349Z",
     "start_time": "2025-10-11T04:15:33.068353Z"
    }
   },
   "source": [
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"RTX4060\"\n",
    "batch_sizes = [1,8,32,128]\n",
    "outputs = []"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1ca2a3cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T04:15:33.823468Z",
     "start_time": "2025-10-11T04:15:33.103722Z"
    }
   },
   "source": [
    "# warmup\n",
    "# context 32K\n",
    "# Internet Access NO\n",
    "\n",
    "def bench(prompts,prefix = \"\", stream = False):\n",
    "    result = {}\n",
    "    output = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prefix},\n",
    "        ] + [{\"role\": \"user\", \"content\": prompt+\"\\n\"} for prompt in prompts],\n",
    "        stream = stream\n",
    "    )\n",
    "\n",
    "    result[\"prompts\"] = len(prompts)\n",
    "    result[\"input_tokens\"] = output.prompt_eval_count\n",
    "    result[\"output_tokens\"] = output.eval_count\n",
    "    result[\"total_time\"] = output.total_duration/1e+6\n",
    "    result[\"load_time\"] = output.load_duration/1e+6\n",
    "    result[\"prompt_eval_time\"] = output.prompt_eval_duration/1e+6\n",
    "    result[\"output_eval_time\"] = output.eval_duration/1e+6\n",
    "\n",
    "    return result, output.message.content\n",
    "\n",
    "bench([\"Do you know whos dashing, smart and disappears in a flash\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'prompts': 1,\n",
       "  'input_tokens': 25,\n",
       "  'output_tokens': 12,\n",
       "  'total_time': 706.1188,\n",
       "  'load_time': 145.9596,\n",
       "  'prompt_eval_time': 3.1081,\n",
       "  'output_eval_time': 555.222},\n",
       " 'I think I do!\\n\\nIs the answer... Superman?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-11T04:15:33.871112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = os.listdir(\"data/prompts/\")\n",
    "\n",
    "# iterating over different files full of variations of questions\n",
    "for filename in files:\n",
    "    print(f\"\\n\\nFile : {filename}\")\n",
    "    # getting some data about the question out of the name of the file\n",
    "    _, questions, prompt_complexity, prompt_length = filename.split(\".\")[0].split(\"_\")\n",
    "\n",
    "    # iterating over different batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nBatch Size : {batch_size}\")\n",
    "        with open(f\"./data/prompts/{filename}\", \"r\") as file:\n",
    "            all_prompts = file.read().split(\"\\\\n\")\n",
    "            all_prompts.pop(-1)\n",
    "\n",
    "            if batch_size == 1:\n",
    "                all_prompts = all_prompts[:32]\n",
    "\n",
    "            for i in range(len(all_prompts)):\n",
    "                all_prompts[i] = f\"Question {i+1} : {all_prompts[i].strip()}\"\n",
    "\n",
    "        # iterating over the prompts in the files in batches of batch_size\n",
    "\n",
    "        for i in tqdm(range(0, len(all_prompts), batch_size)):\n",
    "            prompts = all_prompts[i: i+batch_size]\n",
    "            # iterating over different prefix expecting the output length to be different\n",
    "            for output_length in [\"short\", \"long\"]:\n",
    "                if output_length == \"short\":\n",
    "                    benchmark, answer = bench(prompts, prefix = f\"give concise answers (less than 50 words) per question\")\n",
    "                else:\n",
    "                    benchmark, answer = bench(prompts, prefix = f\"don't ask return questions and just straight up answer the following questions in detail\")\n",
    "                # saving\n",
    "                with open(f\"./data/answers/answer_{batch_size}_{prompt_complexity}_{prompt_length}.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(answer)\n",
    "\n",
    "                benchmark[\"prompt_complexity\"] = prompt_complexity\n",
    "                benchmark[\"prompt_length\"] = prompt_length\n",
    "                benchmark[\"output_length\"] = output_length\n",
    "                benchmark[\"device\"] = device\n",
    "\n",
    "                outputs.append(benchmark)\n",
    "                outputs_df = pd.DataFrame(outputs)\n",
    "                outputs_df.to_csv(f\"./data/{device}_outputs.csv\", index = False)"
   ],
   "id": "7a135d55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File : prompts_128_easy_long.txt\n",
      "\n",
      "Batch Size : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 15/32 [01:58<01:59,  7.00s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outputs_df = pd.read_csv(f\"./data/{device}_outputs.csv\")\n",
    "outputs_df.info()"
   ],
   "id": "8cff8bcb97ab3937",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
