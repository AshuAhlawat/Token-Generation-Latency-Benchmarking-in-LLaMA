{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb29ad36ab22a09",
   "metadata": {},
   "source": [
    "# Why do we need these bench marks?\n",
    "- to predict expected performance\n",
    "- to judge the condition of our system"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd8ddad5ecf64afd",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T18:01:59.047584Z",
     "start_time": "2025-10-16T18:01:58.991508Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "def process(d):\n",
    "    df = pd.read_csv(f\"./data/{d}_outputs.csv\")\n",
    "\n",
    "    df[\"input_tokens_per_prompt\"] = df[\"input_tokens\"] / df[\"prompts\"]\n",
    "    df[\"output_tokens_per_prompt\"] = df[\"output_tokens\"] / df[\"prompts\"]\n",
    "    df[\"prompts\"] = df[\"prompts\"].astype(str)\n",
    "\n",
    "    df[\"load_time\"] = df[\"load_time\"]/1000\n",
    "    df[\"prompt_eval_time\"] = df[\"prompt_eval_time\"]/1000\n",
    "    df[\"output_eval_time\"] = df[\"output_eval_time\"]/1000\n",
    "    df[\"total_time\"] = df[\"total_time\"]/1000\n",
    "\n",
    "    df[\"load_time_per_input_token\"] = df[\"load_time\"] / df[\"input_tokens\"]\n",
    "    df[\"prompt_eval_time_per_input_token\"] = df[\"prompt_eval_time\"] / df[\"input_tokens\"]\n",
    "    df[\"output_eval_time_per_output_token\"] = df[\"output_eval_time\"] / df[\"output_tokens\"]\n",
    "    df[\"total_time_per_input_token\"] = df[\"total_time\"] / df[\"input_tokens\"]\n",
    "    df[\"total_time_per_output_token\"] = df[\"total_time\"] / df[\"output_tokens\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "rtx_df = process(\"RTX4060\")\n",
    "u9_df = process(\"Ultra9-185H\")\n",
    "m3_df = process(\"M3\")\n",
    "\n",
    "all_df = pd.concat([rtx_df, u9_df, m3_df], ignore_index=True)\n",
    "\n",
    "device, all_df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1272 entries, 0 to 1271\n",
      "Data columns (total 18 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   prompts                            1272 non-null   object \n",
      " 1   input_tokens                       1272 non-null   int64  \n",
      " 2   output_tokens                      1272 non-null   int64  \n",
      " 3   total_time                         1272 non-null   float64\n",
      " 4   load_time                          1272 non-null   float64\n",
      " 5   prompt_eval_time                   1272 non-null   float64\n",
      " 6   output_eval_time                   1272 non-null   float64\n",
      " 7   prompt_complexity                  1272 non-null   object \n",
      " 8   prompt_length                      1272 non-null   object \n",
      " 9   output_length                      1272 non-null   object \n",
      " 10  device                             1272 non-null   object \n",
      " 11  input_tokens_per_prompt            1272 non-null   float64\n",
      " 12  output_tokens_per_prompt           1272 non-null   float64\n",
      " 13  load_time_per_input_token          1272 non-null   float64\n",
      " 14  prompt_eval_time_per_input_token   1272 non-null   float64\n",
      " 15  output_eval_time_per_output_token  1272 non-null   float64\n",
      " 16  total_time_per_input_token         1272 non-null   float64\n",
      " 17  total_time_per_output_token        1272 non-null   float64\n",
      "dtypes: float64(11), int64(2), object(5)\n",
      "memory usage: 179.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "4d7e0c353bc3eb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:01:59.079836Z",
     "start_time": "2025-10-16T18:01:59.054676Z"
    }
   },
   "source": [
    "columns = [\"input_tokens\", \"output_tokens\", \"device\", \"total_time\"]\n",
    "\n",
    "inputs_df = all_df[columns]\n",
    "devices = inputs_df[\"device\"].unique()\n",
    "\n",
    "m = inputs_df[[\"input_tokens\", \"output_tokens\"]].mean(numeric_only=True)\n",
    "s = inputs_df[[\"input_tokens\", \"output_tokens\"]].std(numeric_only=True)\n",
    "\n",
    "inputs_df [[\"input_tokens\", \"output_tokens\"]] =  (inputs_df[[\"input_tokens\", \"output_tokens\"]] - m)/s\n",
    "inputs_df = pd.get_dummies(inputs_df)\n",
    "outputs_df = inputs_df.pop(\"total_time\")\n",
    "\n",
    "\n",
    "inputs_arr = np.array(inputs_df, dtype = np.float32)\n",
    "inputs = torch.from_numpy(inputs_arr).to(device)\n",
    "\n",
    "outputs_arr = np.array(outputs_df, dtype = np.float32).reshape(-1,1)\n",
    "outputs = torch.from_numpy(outputs_arr).to(device)\n",
    "inputs_df.columns, inputs.shape, outputs.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['input_tokens', 'output_tokens', 'device_M3', 'device_RTX4060',\n",
       "        'device_Ultra9-185H'],\n",
       "       dtype='object'),\n",
       " torch.Size([1272, 5]),\n",
       " torch.Size([1272, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "db33718dda611466",
   "metadata": {},
   "source": [
    "## Simple 3 layer feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e3e05948f587461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:01:59.119775Z",
     "start_time": "2025-10-16T18:01:59.095684Z"
    }
   },
   "source": [
    "network = nn.Sequential(\n",
    "    nn.Linear(inputs.shape[-1], 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "lossfn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(network.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "cace8d77617e3c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:03:30.987519Z",
     "start_time": "2025-10-16T18:03:21.031991Z"
    }
   },
   "source": [
    "epochs = 5000\n",
    "for i in tqdm(range(epochs)):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    preds = network(inputs)\n",
    "    loss = lossfn(preds, outputs)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1006.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.3811, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:04:33.834367Z",
     "start_time": "2025-10-16T18:04:33.813420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save model\n",
    "PATH = \"./latency_model.pth\"\n",
    "# torch.save(network.state_dict(), PATH)\n",
    "network.load_state_dict(torch.load(PATH))"
   ],
   "id": "62ecb87548490789",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "9be9351404d603e9",
   "metadata": {},
   "source": [
    "## Just enter expected Input and Output tokens to predict the time it would take, without actually sitting there"
   ]
  },
  {
   "cell_type": "code",
   "id": "b928b898a9c36e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T18:05:11.829207Z",
     "start_time": "2025-10-16T18:05:11.817563Z"
    }
   },
   "source": [
    "input_tokens = 92\n",
    "output_tokens = 441\n",
    "\n",
    "input_tokens = (input_tokens - m[\"input_tokens\"])/ s[\"input_tokens\"]\n",
    "output_tokens = (output_tokens  - m[\"output_tokens\"])/ s[\"output_tokens\"]\n",
    "\n",
    "input_ = torch.tensor([[input_tokens, output_tokens, 1, 0, 0],[input_tokens, output_tokens, 0, 1, 0],[input_tokens, output_tokens, 0, 0, 1]], dtype= torch.float32).to(device)\n",
    "output_ = network(input_)\n",
    "\n",
    "print(\"Prediction Time:\")\n",
    "print(f\"M3 : {output_[0,0].item():.2f}s\")\n",
    "print(f\"RTX4060 : {output_[1,0].item():.2f}s\")\n",
    "print(f\"Ultra9 : {output_[2,0].item():.2f}s\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Time:\n",
      "M3 : 27.94s\n",
      "RTX4060 : 9.52s\n",
      "Ultra9 : 56.06s\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
