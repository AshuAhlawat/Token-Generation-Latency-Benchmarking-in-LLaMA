{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Why do we need these bench marks?\n",
    "- to predict expected performance\n",
    "- to judge the condition of our system"
   ],
   "id": "6cb29ad36ab22a09"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-12T18:11:44.547695Z",
     "start_time": "2025-10-12T18:11:41.255750Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1272 entries, 0 to 1271\n",
      "Data columns (total 18 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   prompts                            1272 non-null   object \n",
      " 1   input_tokens                       1272 non-null   int64  \n",
      " 2   output_tokens                      1272 non-null   int64  \n",
      " 3   total_time                         1272 non-null   float64\n",
      " 4   load_time                          1272 non-null   float64\n",
      " 5   prompt_eval_time                   1272 non-null   float64\n",
      " 6   output_eval_time                   1272 non-null   float64\n",
      " 7   prompt_complexity                  1272 non-null   object \n",
      " 8   prompt_length                      1272 non-null   object \n",
      " 9   output_length                      1272 non-null   object \n",
      " 10  device                             1272 non-null   object \n",
      " 11  input_tokens_per_prompt            1272 non-null   float64\n",
      " 12  output_tokens_per_prompt           1272 non-null   float64\n",
      " 13  load_time_per_input_token          1272 non-null   float64\n",
      " 14  prompt_eval_time_per_input_token   1272 non-null   float64\n",
      " 15  output_eval_time_per_output_token  1272 non-null   float64\n",
      " 16  total_time_per_input_token         1272 non-null   float64\n",
      " 17  total_time_per_output_token        1272 non-null   float64\n",
      "dtypes: float64(11), int64(2), object(5)\n",
      "memory usage: 179.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1,
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process(d):\n",
    "    df = pd.read_csv(f\"./data/{d}_outputs.csv\")\n",
    "\n",
    "    df[\"input_tokens_per_prompt\"] = df[\"input_tokens\"] / df[\"prompts\"]\n",
    "    df[\"output_tokens_per_prompt\"] = df[\"output_tokens\"] / df[\"prompts\"]\n",
    "    df[\"prompts\"] = df[\"prompts\"].astype(str)\n",
    "\n",
    "    df[\"load_time\"] = df[\"load_time\"]/1000\n",
    "    df[\"prompt_eval_time\"] = df[\"prompt_eval_time\"]/1000\n",
    "    df[\"output_eval_time\"] = df[\"output_eval_time\"]/1000\n",
    "    df[\"total_time\"] = df[\"total_time\"]/1000\n",
    "\n",
    "    df[\"load_time_per_input_token\"] = df[\"load_time\"] / df[\"input_tokens\"]\n",
    "    df[\"prompt_eval_time_per_input_token\"] = df[\"prompt_eval_time\"] / df[\"input_tokens\"]\n",
    "    df[\"output_eval_time_per_output_token\"] = df[\"output_eval_time\"] / df[\"output_tokens\"]\n",
    "    df[\"total_time_per_input_token\"] = df[\"total_time\"] / df[\"input_tokens\"]\n",
    "    df[\"total_time_per_output_token\"] = df[\"total_time\"] / df[\"output_tokens\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "rtx_df = process(\"RTX4060\")\n",
    "u9_df = process(\"Ultra9-185H\")\n",
    "m3_df = process(\"M3\")\n",
    "\n",
    "all_df = pd.concat([rtx_df, u9_df, m3_df], ignore_index=True)\n",
    "\n",
    "device, all_df.info()"
   ],
   "id": "dd8ddad5ecf64afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:11:44.658282Z",
     "start_time": "2025-10-12T18:11:44.547695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"input_tokens\", \"output_tokens\", \"device\", \"total_time\"]\n",
    "\n",
    "inputs_df = all_df[columns]\n",
    "devices = inputs_df[\"device\"].unique()\n",
    "\n",
    "m = inputs_df[[\"input_tokens\", \"output_tokens\"]].mean(numeric_only=True)\n",
    "s = inputs_df[[\"input_tokens\", \"output_tokens\"]].std(numeric_only=True)\n",
    "\n",
    "inputs_df [[\"input_tokens\", \"output_tokens\"]] =  (inputs_df[[\"input_tokens\", \"output_tokens\"]] - m)/s\n",
    "inputs_df = pd.get_dummies(inputs_df)\n",
    "outputs_df = inputs_df.pop(\"total_time\")\n",
    "\n",
    "\n",
    "inputs_arr = np.array(inputs_df, dtype = np.float32)\n",
    "inputs = torch.from_numpy(inputs_arr).to(device)\n",
    "\n",
    "outputs_arr = np.array(outputs_df, dtype = np.float32).reshape(-1,1)\n",
    "outputs = torch.from_numpy(outputs_arr).to(device)\n",
    "inputs_df.columns, inputs.shape, outputs.shape"
   ],
   "id": "4d7e0c353bc3eb1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['input_tokens', 'output_tokens', 'device_M3', 'device_RTX4060',\n",
       "        'device_Ultra9-185H'],\n",
       "       dtype='object'),\n",
       " torch.Size([1272, 5]),\n",
       " torch.Size([1272, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple 3 layer feed-forward Neural Network",
   "id": "db33718dda611466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:11:46.096235Z",
     "start_time": "2025-10-12T18:11:44.664688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "network = nn.Sequential(\n",
    "    nn.Linear(inputs.shape[-1], 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "lossfn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(network.parameters(), lr=0.001)"
   ],
   "id": "6e3e05948f587461",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:12:06.462921Z",
     "start_time": "2025-10-12T18:11:46.096235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10000\n",
    "for i in tqdm(range(epochs)):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    preds = network(inputs)\n",
    "    loss = lossfn(preds, outputs)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(loss)"
   ],
   "id": "cace8d77617e3c85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 496.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(88.5694, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Just enter expected Input and Output tokens to predict the time it would take, without actually sitting there",
   "id": "9be9351404d603e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:12:06.526504Z",
     "start_time": "2025-10-12T18:12:06.462921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_tokens = 92\n",
    "output_tokens = 441\n",
    "\n",
    "input_tokens = (input_tokens - m[\"input_tokens\"])/ s[\"input_tokens\"]\n",
    "output_tokens = (output_tokens  - m[\"output_tokens\"])/ s[\"output_tokens\"]\n",
    "\n",
    "input_ = torch.tensor([[input_tokens, output_tokens, 1, 0, 0],[input_tokens, output_tokens, 0, 1, 0],[input_tokens, output_tokens, 0, 0, 1]], dtype= torch.float32).to(device)\n",
    "output_ = network(input_)\n",
    "\n",
    "print(\"Prediction Time:\")\n",
    "print(f\"M3 : {output_[0,0].item():.2f}s\")\n",
    "print(f\"RTX4060 : {output_[1,0].item():.2f}s\")\n",
    "print(f\"Ultra9 : {output_[2,0].item():.2f}s\")"
   ],
   "id": "b928b898a9c36e3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Time:\n",
      "M3 : 27.96s\n",
      "RTX4060 : 0.00s\n",
      "Ultra9 : 56.76s\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
